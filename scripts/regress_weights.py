"""
WCS v3 - SHAP + XGBoost ç‰ˆæœ¬
ç”¨ XGBoost é¢„æµ‹èƒœè´Ÿï¼ŒSHAP è§£é‡Šæ¯ä¸ªç‰¹å¾çš„è´¡çŒ®
æœ€ç»ˆè¾“å‡ºï¼šåˆ†ç±»åˆ«çš„ SHAP æƒé‡ + å¯ç›´æ¥å†™å…¥ config.js çš„é…ç½®

ä½¿ç”¨: pixi run regress  (æˆ– python scripts/regress_weights.py wcs_raw_data.json)
"""
import json
import sys
import numpy as np
from pathlib import Path
from collections import defaultdict

def main():
    # ===== åŠ è½½æ•°æ® =====
    data_path = sys.argv[1] if len(sys.argv) > 1 else 'wcs_raw_data.json'
    for p in [data_path, f'scripts/{data_path}', f'../{data_path}']:
        if Path(p).exists():
            data_path = p
            break

    with open(data_path, 'r', encoding='utf-8') as f:
        raw = json.load(f)

    dataset = raw['dataset']

    # ===== ä¿®æ­£ isWin bug =====
    # collect-data.js ä¸­ match.WinnerTeam ç¼–å·å¯èƒ½ä¸ TeamId ä¸ä¸€è‡´
    # ç”¨ ratingDelta > 0 = èµ¢ æ¥è¦†ç›–åŸå§‹ isWin
    fixed = 0
    for d in dataset:
        correct = 1 if d.get('ratingDelta', 0) > 0 else 0
        if d['isWin'] != correct:
            fixed += 1
            d['isWin'] = correct

    # ç‰¹å¾å®šä¹‰ï¼ˆæ’é™¤æ ‡ç­¾å’Œéç‰¹å¾å­—æ®µï¼‰
    exclude = {'matchId', 'playerId', 'teamId', 'isWin',
               'oldRating', 'newRating', 'ratingDelta'}
    feature_names = [k for k in dataset[0].keys() if k not in exclude]

    X_raw = np.array([[d[f] for f in feature_names] for d in dataset])
    y = np.array([d['isWin'] for d in dataset])

    print(f"\n{'='*70}")
    print(f"ğŸ“Š WCS v3 â€” SHAP + XGBoost åˆ†æ")
    print(f"{'='*70}")
    print(f"  æ ·æœ¬: {len(dataset)}  |  å¯¹å±€: {raw['metadata']['matchCount']}")
    print(f"  ç‰¹å¾: {len(feature_names)}  |  èƒœ/è´¥: {y.sum()}/{len(y)-y.sum()}")

    # ===== æŒ‰ matchId åšç™¾åˆ†ä½åŒ– =====
    match_groups = defaultdict(list)
    for i, d in enumerate(dataset):
        match_groups[d['matchId']].append(i)

    X_pct = np.zeros_like(X_raw, dtype=np.float64)
    for indices in match_groups.values():
        for fi in range(len(feature_names)):
            vals = X_raw[indices, fi]
            for idx in indices:
                v = X_raw[idx, fi]
                below = np.sum(vals < v)
                equal = np.sum(vals == v)
                X_pct[idx, fi] = (below + equal * 0.5) / len(vals)

    # è¿‡æ»¤é›¶æ–¹å·®ç‰¹å¾
    valid = np.std(X_pct, axis=0) > 1e-8
    removed = [f for f, v in zip(feature_names, valid) if not v]
    if removed:
        print(f"  âš ï¸ ç§»é™¤é›¶æ–¹å·®: {', '.join(removed)}")
    feature_names = [f for f, v in zip(feature_names, valid) if v]
    X_pct = X_pct[:, valid]

    # ===== XGBoost è®­ç»ƒ =====
    from xgboost import XGBClassifier
    from sklearn.model_selection import cross_val_score

    model = XGBClassifier(
        n_estimators=200,
        max_depth=4,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        use_label_encoder=False,
        eval_metric='logloss',
    )

    cv_scores = cross_val_score(model, X_pct, y, cv=5, scoring='accuracy')
    print(f"\n  XGBoost 5-fold CV: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")

    model.fit(X_pct, y)

    # ===== SHAP åˆ†æ =====
    import shap
    print(f"\n  è®¡ç®— SHAP å€¼ä¸­...")

    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X_pct)

    # shap_values å½¢çŠ¶: (n_samples, n_features)
    # æ­£å€¼ = å€¾å‘èƒœåˆ©, è´Ÿå€¼ = å€¾å‘å¤±è´¥

    # ===== å…¨å±€ç‰¹å¾é‡è¦æ€§ï¼ˆå¹³å‡ |SHAP|ï¼‰=====
    mean_abs_shap = np.mean(np.abs(shap_values), axis=0)
    mean_shap = np.mean(shap_values, axis=0)  # å¸¦æ–¹å‘çš„å¹³å‡

    print(f"\n{'='*70}")
    print(f"ğŸ”« å…¨å±€ SHAP ç‰¹å¾é‡è¦æ€§ (mean |SHAP|)")
    print(f"{'='*70}")

    importance = sorted(zip(feature_names, mean_abs_shap, mean_shap),
                       key=lambda x: -x[1])
    for fname, imp, direction in importance:
        bar = "â–ˆ" * int(imp * 80)
        sign = "â†‘" if direction > 0 else "â†“"
        print(f"  {sign} {fname:22s}: {imp:.4f}  {bar}")

    # ===== åˆ†ç±»åˆ«æ±‡æ€» =====
    CATEGORIES = {
        'ç»æµç®¡ç†': ['totalSpawned', 'totalRefunded', 'refundRate', 'netInvestment',
                   'supplyConsumed'],
        'æˆ˜æ–—æ•ˆç‡': ['costEfficiency', 'damageTrade', 'dlRatio', 'survivalRate'],
        'ç«åŠ›è¾“å‡º': ['damageDealt', 'destructionScore', 'damageReceived'],
        'æˆ˜åœºè´¡çŒ®': ['lossesScore', 'teamLossShare', 'teamDmgShare',
                   'teamDestShare', 'teamSpawnShare'],
        'æˆ˜ç•¥ç›®æ ‡': ['objectivesCaptured', 'supplyCaptured', 'buildingsDestroyed'],
        'å›¢é˜Ÿåä½œ': ['supplyFromAllies', 'supplyToAllies', 'uniqueUnits', 'unitCount'],
    }

    # å»ºç«‹ feature_name -> index æ˜ å°„
    fname_to_idx = {f: i for i, f in enumerate(feature_names)}

    print(f"\n{'='*70}")
    print(f"ğŸ“Š åˆ†ç±»åˆ« SHAP è´¡çŒ® (ç”¨äºé›·è¾¾å›¾)")
    print(f"{'='*70}")

    category_importance = {}
    for cat, features in CATEGORIES.items():
        cat_shap = 0
        cat_features = []
        for f in features:
            if f in fname_to_idx:
                idx = fname_to_idx[f]
                cat_shap += mean_abs_shap[idx]
                cat_features.append((f, mean_abs_shap[idx], mean_shap[idx]))
        category_importance[cat] = {
            'total': cat_shap,
            'features': cat_features,
        }

    # å½’ä¸€åŒ–ä¸ºç™¾åˆ†æ¯”
    total_imp = sum(v['total'] for v in category_importance.values())
    print()
    for cat in sorted(category_importance, key=lambda c: -category_importance[c]['total']):
        info = category_importance[cat]
        pct = info['total'] / total_imp * 100 if total_imp > 0 else 0
        bar = "â–ˆ" * int(pct * 0.6)
        print(f"  {cat:10s}: {pct:5.1f}%  {bar}")
        for f, imp, direction in sorted(info['features'], key=lambda x: -x[1]):
            sign = "+" if direction > 0 else "-"
            print(f"    {sign} {f:20s}: {imp:.4f}")

    # ===== èƒœæ–¹ vs è´¥æ–¹çš„ SHAP å¯¹æ¯” =====
    print(f"\n{'='*70}")
    print(f"ğŸ† èƒœæ–¹ vs è´¥æ–¹çš„å¹³å‡ SHAP å€¼")
    print(f"{'='*70}")

    win_mask = y == 1
    lose_mask = y == 0
    win_shap = np.mean(shap_values[win_mask], axis=0)
    lose_shap = np.mean(shap_values[lose_mask], axis=0)

    diffs = sorted(zip(feature_names, win_shap, lose_shap),
                  key=lambda x: -(x[1] - x[2]))
    print(f"\n  {'ç‰¹å¾':22s}  {'èƒœæ–¹':>8s}  {'è´¥æ–¹':>8s}  {'å·®å¼‚':>8s}")
    print(f"  {'-'*52}")
    for fname, w, l in diffs:
        diff = w - l
        indicator = "â¬†ï¸" if diff > 0.01 else ("â¬‡ï¸" if diff < -0.01 else "  ")
        print(f"  {fname:22s}  {w:+.4f}  {l:+.4f}  {diff:+.4f} {indicator}")

    # ===== ç”Ÿæˆ WCS é…ç½®æƒé‡ =====
    print(f"\n{'='*70}")
    print(f"ğŸ“‹ WCS é…ç½®å»ºè®® (åŸºäº SHAP åˆ†ç±»åˆ«æƒé‡)")
    print(f"{'='*70}")

    # æ­£å‘ç±»åˆ« = SHAP mean > 0 çš„ç‰¹å¾çš„é‡è¦æ€§
    wcs_dims = {}
    for cat, info in category_importance.items():
        wcs_dims[cat] = info['total'] / total_imp if total_imp > 0 else 0

    print(f"\n  WCS ç»´åº¦æƒé‡ (å½’ä¸€åŒ–, ç•™ 15% ç»™ winBonus):")
    for cat in sorted(wcs_dims, key=lambda c: -wcs_dims[c]):
        w = wcs_dims[cat] * 0.85
        print(f"    {cat:12s}: {w:.4f}")
    print(f"    {'winBonus':12s}: 0.1500")

    # ===== SHAP äº¤äº’æ•ˆåº” (top äº¤äº’å¯¹) =====
    print(f"\n{'='*70}")
    print(f"ğŸ”— SHAP ç‰¹å¾äº¤äº’åˆ†æ (Top 10)")
    print(f"{'='*70}")

    # ä½¿ç”¨ç‰¹å¾é‡è¦æ€§çš„åæ–¹å·®è¿‘ä¼¼äº¤äº’
    shap_cov = np.abs(np.corrcoef(shap_values.T))
    interactions = []
    for i in range(len(feature_names)):
        for j in range(i+1, len(feature_names)):
            interactions.append((feature_names[i], feature_names[j], shap_cov[i, j]))
    interactions.sort(key=lambda x: -x[2])

    print()
    for f1, f2, strength in interactions[:10]:
        bar = "â–ˆ" * int(strength * 30)
        print(f"  {f1:20s} Ã— {f2:20s}: {strength:.3f}  {bar}")

    # ===== ä¿å­˜ç»“æœ =====
    result = {
        'method': 'XGBoost + SHAP',
        'cv_accuracy': float(cv_scores.mean()),
        'cv_std': float(cv_scores.std()),
        'global_shap_importance': {f: float(v) for f, v in zip(feature_names, mean_abs_shap)},
        'global_shap_direction': {f: float(v) for f, v in zip(feature_names, mean_shap)},
        'category_weights': {cat: float(info['total'] / total_imp) for cat, info in category_importance.items()},
        'win_vs_lose_shap': {
            f: {'win': float(w), 'lose': float(l), 'diff': float(w-l)}
            for f, w, l in diffs
        },
        'sample_count': len(dataset),
        'match_count': raw['metadata']['matchCount'],
    }

    out_path = Path(data_path).parent / 'wcs_shap_analysis.json'
    with open(out_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    print(f"\nâœ… åˆ†æç»“æœå·²ä¿å­˜åˆ° {out_path}")

    # ===== SHAP Summary Plot =====
    try:
        print(f"\nğŸ“Š æ­£åœ¨ç”Ÿæˆ SHAP Summary Plot...")
        import matplotlib
        matplotlib.use('Agg')
        import matplotlib.pyplot as plt

        fig, ax = plt.subplots(figsize=(12, 8))
        shap.summary_plot(shap_values, X_pct, feature_names=feature_names,
                         show=False, max_display=20)
        plot_path = Path(data_path).parent / 'shap_summary.png'
        plt.tight_layout()
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"  ğŸ“ˆ SHAP å›¾å·²ä¿å­˜åˆ° {plot_path}")
    except Exception as e:
        print(f"  âš ï¸ å›¾è¡¨ç”Ÿæˆå¤±è´¥ (å¯é€‰): {e}")

if __name__ == '__main__':
    main()
